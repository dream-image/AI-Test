import { pipeline, TextStreamer, env } from '@huggingface/transformers';

// æ€§èƒ½ä¼˜åŒ–é…ç½®
env.allowLocalModels = false;
env.backends.onnx.wasm.numThreads = 4; // å‡å°‘çº¿ç¨‹é¿å…å†…å­˜é—®é¢˜
env.backends.onnx.wasm.proxy = false;

class MyTextGenerationPipeline {
    static task = 'text-generation';
    // ğŸš€ SmolLM3-3B-Base - 3Bæ¨¡å‹ï¼Œä½¿ç”¨å¼ºåŠ›é‡åŒ–
    static model = 'HuggingFaceTB/SmolLM3-3B-Base';
    static instance: any = null;

    static async getInstance(progress_callback: ((data: any) => void) | null = null) {
        if (this.instance === null) {
            this.instance = pipeline(this.task as any, this.model, {
                progress_callback: progress_callback as any,
                // ğŸ”¥ å…³é”®ï¼šä½¿ç”¨ q4 å¼ºåŠ›é‡åŒ–å‡å°‘å†…å­˜å ç”¨
                dtype: 'q4',
                // ğŸ”¥ ä¼˜å…ˆä½¿ç”¨ WebGPU åŠ é€Ÿï¼ˆå¦‚æœå¯ç”¨ï¼‰
                device: 'auto',
            });
        }
        return this.instance;
    }
}

// Listen for messages from the main thread
self.addEventListener("message", async (event) => {
    const generator = await MyTextGenerationPipeline.getInstance((x) => {
        self.postMessage(x);
    });

    self.postMessage({ status: 'ready' });

    // ç›´æ¥ä½¿ç”¨æç¤ºè¯
    const userPrompt = event.data.text;

    // æµå¼è¾“å‡º
    const streamer = new TextStreamer(generator.tokenizer, {
        skip_prompt: true,
        skip_special_tokens: true,
        callback_function: function (text: string) {
            self.postMessage({
                status: "update",
                output: text,
            });
        },
    });

    // ç”Ÿæˆå‚æ•°ï¼ˆä¼˜åŒ–é…ç½®ï¼‰
    const output = await generator(userPrompt, {
        max_new_tokens: event.data.max_new_tokens || 100, // å‡å°‘é•¿åº¦æå‡é€Ÿåº¦
        temperature: 0.7,
        top_k: 50,
        top_p: 0.9,
        do_sample: true,
        streamer,
    });

    self.postMessage({
        status: "complete",
        output: output[0].generated_text,
    });
});
